"""
File operations for scanning directories and generating task files.
"""

import os
import subprocess
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Set

from .utils import print_status, print_progress


def find_files_with_suffixes(directory: str, suffixes: Set[str]) -> List[str]:
    """
    Recursively find all files with the specified suffixes in the directory.
    
    Args:
        directory: Root directory to search
        suffixes: Set of file suffixes to look for
        
    Returns:
        List of file paths matching the suffixes
    """
    matching_files = []
    
    print_status(f"Scanning directory: {os.path.abspath(directory)}")
    print_status(f"Looking for suffixes: {', '.join(sorted(suffixes))}")
    
    try:
        # Count total files for progress tracking
        total_files = sum(len(files) for _, _, files in os.walk(directory))
        scanned_files = 0
        
        for root, dirs, files in os.walk(directory):
            for file in files:
                scanned_files += 1
                print_progress(scanned_files, total_files, "Scanning files")
                
                file_path = os.path.join(root, file)
                # Check if file ends with any of the specified suffixes
                if any(file.endswith(suffix) for suffix in suffixes):
                    matching_files.append(file_path)
        
        print()  # Clear progress bar
        print_status(f"Scan complete! Found {len(matching_files)} matching files out of {total_files} total files")
        
    except Exception as e:
        print(f"\n[ERROR] Error scanning directory {directory}: {e}")
        return []
    
    return sorted(matching_files)


def generate_task_file(files: List[str], output_file: str = "gzip.cmds", max_jobs: int = None, operation_mode: str = "gzip", mode_args = None) -> str:
    """
    Generate a task file with commands for the found files based on operation mode.
    
    Args:
        files: List of file paths to process
        output_file: Name of the output task file
        max_jobs: Maximum number of jobs for chunking (optional)
        operation_mode: Operation mode ("gzip", "gunzip", "sam_to_bam", "bam_to_sam")
        mode_args: Additional arguments for the mode (optional)
        
    Returns:
        Path to the generated task file
    """
    task_file_path = os.path.abspath(output_file)
    
    print_status(f"Generating task file: {task_file_path}")
    
    # If chunking is needed and there are more than max_jobs files
    if max_jobs and len(files) > max_jobs:
        print_status(f"More than {max_jobs} files detected, creating chunked version for SLURM compatibility", "[INFO]")
        
        # Calculate optimal chunking
        commands_per_job = max(1, (len(files) + max_jobs - 1) // max_jobs)
        actual_jobs = (len(files) + commands_per_job - 1) // commands_per_job
        
        print_status(f"Restructuring for SLURM array limit: {max_jobs} max jobs", "[INFO]")
        print_status(f"Files to compress: {len(files)}", "[INFO]")
        print_status(f"Jobs to submit: {actual_jobs}", "[INFO]")
        print_status(f"Commands per job: {commands_per_job}", "[INFO]")
        
        with open(task_file_path, 'w') as f:
            mode_description = get_mode_description(operation_mode)
            f.write(f"# {mode_description} task file generated by gzip-up.py (chunked mode)\n")
            f.write("# Each line contains multiple commands separated by semicolons\n")
            f.write("# Run with Slurm job arrays - each task processes multiple files\n\n")
            
            skipped_count = 0
            for i in range(0, len(files), commands_per_job):
                chunk = files[i:i + commands_per_job]
                chunk_commands = []
                
                for file_path in chunk:
                    # Skip if file should be skipped based on mode
                    if should_skip_file(file_path, operation_mode):
                        skipped_count += 1
                        continue
                    
                    # Add command to chunk based on mode
                    command = generate_command(file_path, operation_mode, mode_args)
                    if command:
                        chunk_commands.append(command)
                
                # Write chunk if it contains any commands
                if chunk_commands:
                    combined_cmd = "; ".join(chunk_commands)
                    f.write(f"{combined_cmd}\n")
                
                # Show progress
                print_progress(i + len(chunk), len(files), "Writing chunked commands")
            
            print()  # Clear progress bar
            
            if skipped_count > 0:
                print_status(f"Skipped {skipped_count} files that don't need processing", "[WARN]")
            
            print_status(f"Chunked task file created with {actual_jobs} job chunks", "[OK]")
            return task_file_path
    
    # Standard non-chunked mode
    with open(task_file_path, 'w') as f:
        mode_description = get_mode_description(operation_mode)
        f.write(f"# {mode_description} task file generated by gzip-up.py\n")
        f.write("# Each line contains a command to process a file\n")
        f.write("# Run with: parallel < gzip.cmds\n")
        f.write("# Or use with Slurm: srun --multi-prog gzip.cmds\n\n")
        
        skipped_count = 0
        for i, file_path in enumerate(files):
            # Skip if file should be skipped based on mode
            if should_skip_file(file_path, operation_mode):
                skipped_count += 1
                continue
                
            # Create command based on mode
            command = generate_command(file_path, operation_mode, mode_args)
            if command:
                f.write(f"{command}\n")
            
            # Show progress
            print_progress(i + 1, len(files), "Writing commands")
    
    print()  # Clear progress bar
    
    commands_written = len(files) - skipped_count
    if skipped_count > 0:
        print_status(f"Skipped {skipped_count} files that don't need processing", "[WARN]")
    
    print_status(f"Task file created with {commands_written} commands", "[OK]")
    
    return task_file_path


def execute_gzip_local(files: List[str], num_threads: int = 1, operation_mode: str = "gzip", mode_args = None) -> dict:
    """
    Execute file operations locally using threading.
    
    Args:
        files: List of file paths to process
        num_threads: Number of threads to use (0 for auto-detect)
        operation_mode: Operation mode ("gzip", "gunzip", "sam_to_bam", "bam_to_sam")
        mode_args: Additional arguments for the mode (optional)
        
    Returns:
        Dictionary with execution results
    """
    if num_threads == 0:
        # Auto-detect based on available cores (but cap at 8 for I/O operations)
        import multiprocessing
        num_threads = min(multiprocessing.cpu_count(), 8)
    
    if num_threads < 1:
        num_threads = 1
    
    mode_description = get_mode_description(operation_mode)
    print_status(f"Starting local {mode_description.lower()} execution with {num_threads} threads")
    
    # Filter out files that should be skipped based on mode
    processable_files = [f for f in files if not should_skip_file(f, operation_mode)]
    skipped_count = len(files) - len(processable_files)
    
    if skipped_count > 0:
        print_status(f"Skipped {skipped_count} files that don't need processing", "[WARN]")
    
    if not processable_files:
        print_status(f"No files to process", "[WARN]")
        return {
            'total': len(files),
            'processed': 0,
            'skipped': skipped_count,
            'errors': 0,
            'error_files': []
        }
    
    results = {
        'total': len(files),
        'processed': 0,
        'skipped': skipped_count,
        'errors': 0,
        'error_files': []
    }
    
    # Thread-safe counter for progress
    completed_lock = threading.Lock()
    completed_count = 0
    
    def process_file(file_path: str) -> tuple:
        """Process a single file based on the operation mode."""
        nonlocal completed_count
        
        try:
            # Generate command based on mode
            command = generate_command(file_path, operation_mode, mode_args)
            if not command:
                return (file_path, False, "Invalid operation mode")
            
            # Split command if it contains multiple operations (e.g., samtools with &&)
            if ' && ' in command:
                # Execute multiple commands sequentially
                for cmd_part in command.split(' && '):
                    # Use shell=True for commands with quotes and special characters
                    result = subprocess.run(
                        cmd_part.strip(),
                        shell=True,
                        capture_output=True,
                        text=True,
                        check=True
                    )
            else:
                # Execute single command
                # Use shell=True for commands with quotes and special characters
                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    check=True
                )
            
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(processable_files), f"Processing files")
            
            return (file_path, True, None)
            
        except subprocess.CalledProcessError as e:
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(processable_files), f"Processing files")
            
            return (file_path, False, str(e))
        except Exception as e:
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(processable_files), f"Processing files")
            
            return (file_path, False, str(e))
    
    # Execute processing using thread pool
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        # Submit all tasks
        future_to_file = {
            executor.submit(process_file, file_path): file_path 
            for file_path in processable_files
        }
        
        # Process completed tasks
        for future in as_completed(future_to_file):
            file_path, success, error = future.result()
            
            if success:
                results['processed'] += 1
            else:
                results['errors'] += 1
                results['error_files'].append((file_path, error))
    
    print()  # Clear progress bar
    
    # Print summary
    print_status(f"Local execution complete!", "[OK]")
    print_status(f"Files processed: {results['processed']}", "[OK]")
    print_status(f"Files skipped: {results['skipped']}", "[WARN]")
    
    if results['errors'] > 0:
        print_status(f"Errors encountered: {results['errors']}", "[ERROR]")
        for file_path, error in results['error_files'][:5]:  # Show first 5 errors
            print_status(f"  {file_path}: {error}", "[ERROR]")
        if len(results['error_files']) > 5:
            print_status(f"  ... and {len(results['error_files']) - 5} more errors", "[ERROR]")
    
    return results


def get_mode_description(operation_mode: str) -> str:
    """Get a human-readable description of the operation mode."""
    mode_descriptions = {
        "gzip": "Gzip compression",
        "gunzip": "Gunzip decompression", 
        "sam_to_bam": "SAM to BAM conversion",
        "bam_to_sam": "BAM to SAM conversion"
    }
    return mode_descriptions.get(operation_mode, "File processing")


def should_skip_file(file_path: str, operation_mode: str) -> bool:
    """Determine if a file should be skipped based on the operation mode."""
    if operation_mode == "gzip":
        # Skip if file is already compressed
        return file_path.endswith('.gz')
    elif operation_mode == "gunzip":
        # Skip if file is not compressed
        return not file_path.endswith('.gz')
    elif operation_mode == "sam_to_bam":
        # Skip if file is not a SAM file
        return not file_path.endswith('.sam')
    elif operation_mode == "bam_to_sam":
        # Skip if file is not a BAM file
        return not file_path.endswith('.bam')
    return False


def generate_command(file_path: str, operation_mode: str, mode_args=None) -> str:
    """Generate the appropriate command based on the operation mode."""
    if operation_mode == "gzip":
        return f"gzip '{file_path}'"
    elif operation_mode == "gunzip":
        return f"gunzip '{file_path}'"
    elif operation_mode == "sam_to_bam":
        # Generate output filename
        fps = file_path.split('.')
        file_path_prefix, suffix = '.'.join(fps[:-1]), fps[-1]
        assert suffix == 'sam'
        output_file = f"{file_path_prefix}.bam"
        return f"samtools view -bS -o '{output_file}' '{file_path}'"
    elif operation_mode == "bam_to_sam":
        # Generate output filename
        fps = file_path.split('.')
        file_path_prefix, suffix = '.'.join(fps[:-1]), fps[-1]
        assert suffix == 'bam'
        output_file = f"{file_path_prefix}.sam"
        return f"samtools view -h -o '{output_file}' '{file_path}'"
    
    return None
