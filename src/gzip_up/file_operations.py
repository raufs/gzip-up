"""
File operations for scanning directories and generating task files.
"""

import os
import subprocess
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Set

from .utils import print_status, print_progress


def find_files_with_suffixes(directory: str, suffixes: Set[str]) -> List[str]:
    """
    Recursively find all files with the specified suffixes in the directory.
    
    Args:
        directory: Root directory to search
        suffixes: Set of file suffixes to look for
        
    Returns:
        List of file paths matching the suffixes
    """
    matching_files = []
    
    print_status(f"Scanning directory: {os.path.abspath(directory)}")
    print_status(f"Looking for suffixes: {', '.join(sorted(suffixes))}")
    
    try:
        # Count total files for progress tracking
        total_files = sum(len(files) for _, _, files in os.walk(directory))
        scanned_files = 0
        
        for root, dirs, files in os.walk(directory):
            for file in files:
                scanned_files += 1
                print_progress(scanned_files, total_files, "Scanning files")
                
                file_path = os.path.join(root, file)
                # Check if file ends with any of the specified suffixes
                if any(file.endswith(suffix) for suffix in suffixes):
                    matching_files.append(file_path)
        
        print()  # Clear progress bar
        print_status(f"Scan complete! Found {len(matching_files)} matching files out of {total_files} total files")
        
    except Exception as e:
        print(f"\n[ERROR] Error scanning directory {directory}: {e}")
        return []
    
    return sorted(matching_files)


def generate_task_file(files: List[str], output_file: str = "gzip.cmds", max_jobs: int = None) -> str:
    """
    Generate a task file with gzip commands for the found files.
    
    Args:
        files: List of file paths to compress
        output_file: Name of the output task file
        max_jobs: Maximum number of jobs for chunking (optional)
        
    Returns:
        Path to the generated task file
    """
    task_file_path = os.path.abspath(output_file)
    
    print_status(f"Generating task file: {task_file_path}")
    
    # If chunking is needed and there are more than max_jobs files
    if max_jobs and len(files) > max_jobs:
        print_status(f"More than {max_jobs} files detected, creating chunked version for SLURM compatibility", "[INFO]")
        
        # Calculate optimal chunking
        commands_per_job = max(1, (len(files) + max_jobs - 1) // max_jobs)
        actual_jobs = (len(files) + commands_per_job - 1) // commands_per_job
        
        print_status(f"Restructuring for SLURM array limit: {max_jobs} max jobs", "[INFO]")
        print_status(f"Files to compress: {len(files)}", "[INFO]")
        print_status(f"Jobs to submit: {actual_jobs}", "[INFO]")
        print_status(f"Commands per job: {commands_per_job}", "[INFO]")
        
        with open(task_file_path, 'w') as f:
            f.write("# Gzip task file generated by gzip-up.py (chunked mode)\n")
            f.write("# Each line contains multiple gzip commands separated by semicolons\n")
            f.write("# Run with Slurm job arrays - each task processes multiple files\n\n")
            
            skipped_count = 0
            for i in range(0, len(files), commands_per_job):
                chunk = files[i:i + commands_per_job]
                chunk_commands = []
                
                for file_path in chunk:
                    # Skip if file is already compressed
                    if file_path.endswith('.gz'):
                        skipped_count += 1
                        continue
                    
                    # Add gzip command to chunk
                    chunk_commands.append(f"gzip '{file_path}'")
                
                # Write chunk if it contains any commands
                if chunk_commands:
                    combined_cmd = "; ".join(chunk_commands)
                    f.write(f"{combined_cmd}\n")
                
                # Show progress
                print_progress(i + len(chunk), len(files), "Writing chunked commands")
            
            print()  # Clear progress bar
            
            if skipped_count > 0:
                print_status(f"Skipped {skipped_count} already compressed files", "[WARN]")
            
            print_status(f"Chunked task file created with {actual_jobs} job chunks", "[OK]")
            return task_file_path
    
    # Standard non-chunked mode
    with open(task_file_path, 'w') as f:
        f.write("# Gzip task file generated by gzip-up.py\n")
        f.write("# Each line contains a gzip command to compress a file\n")
        f.write("# Run with: parallel < gzip.cmds\n")
        f.write("# Or use with Slurm: srun --multi-prog gzip.cmds\n\n")
        
        skipped_count = 0
        for i, file_path in enumerate(files):
            # Skip if file is already compressed
            if file_path.endswith('.gz'):
                skipped_count += 1
                continue
                
            # Create gzip command
            gzip_cmd = f"gzip '{file_path}'"
            f.write(f"{gzip_cmd}\n")
            
            # Show progress
            print_progress(i + 1, len(files), "Writing commands")
    
    print()  # Clear progress bar
    
    commands_written = len(files) - skipped_count
    if skipped_count > 0:
        print_status(f"Skipped {skipped_count} already compressed files", "[WARN]")
    
    print_status(f"Task file created with {commands_written} gzip commands", "[OK]")
    
    return task_file_path


def execute_gzip_local(files: List[str], num_threads: int = 1) -> dict:
    """
    Execute gzip operations locally using threading.
    
    Args:
        files: List of file paths to compress
        num_threads: Number of threads to use (0 for auto-detect)
        
    Returns:
        Dictionary with execution results
    """
    if num_threads == 0:
        # Auto-detect based on available cores (but cap at 8 for I/O operations)
        import multiprocessing
        num_threads = min(multiprocessing.cpu_count(), 8)
    
    if num_threads < 1:
        num_threads = 1
    
    print_status(f"Starting local gzip execution with {num_threads} threads")
    
    # Filter out already compressed files
    uncompressed_files = [f for f in files if not f.endswith('.gz')]
    skipped_count = len(files) - len(uncompressed_files)
    
    if skipped_count > 0:
        print_status(f"Skipped {skipped_count} already compressed files", "[WARN]")
    
    if not uncompressed_files:
        print_status("No files to compress", "[WARN]")
        return {
            'total': len(files),
            'compressed': 0,
            'skipped': skipped_count,
            'errors': 0,
            'error_files': []
        }
    
    results = {
        'total': len(files),
        'compressed': 0,
        'skipped': skipped_count,
        'errors': 0,
        'error_files': []
    }
    
    # Thread-safe counter for progress
    completed_lock = threading.Lock()
    completed_count = 0
    
    def compress_file(file_path: str) -> tuple:
        """Compress a single file using gzip."""
        nonlocal completed_count
        
        try:
            # Run gzip command
            result = subprocess.run(
                ['gzip', file_path],
                capture_output=True,
                text=True,
                check=True
            )
            
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(uncompressed_files), "Compressing files")
            
            return (file_path, True, None)
            
        except subprocess.CalledProcessError as e:
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(uncompressed_files), "Compressing files")
            
            return (file_path, False, str(e))
        except Exception as e:
            # Update progress
            with completed_lock:
                completed_count += 1
                print_progress(completed_count, len(uncompressed_files), "Compressing files")
            
            return (file_path, False, str(e))
    
    # Execute compression using thread pool
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        # Submit all tasks
        future_to_file = {
            executor.submit(compress_file, file_path): file_path 
            for file_path in uncompressed_files
        }
        
        # Process completed tasks
        for future in as_completed(future_to_file):
            file_path, success, error = future.result()
            
            if success:
                results['compressed'] += 1
            else:
                results['errors'] += 1
                results['error_files'].append((file_path, error))
    
    print()  # Clear progress bar
    
    # Print summary
    print_status(f"Local execution complete!", "[OK]")
    print_status(f"Files compressed: {results['compressed']}", "[OK]")
    print_status(f"Files skipped: {results['skipped']}", "[WARN]")
    
    if results['errors'] > 0:
        print_status(f"Errors encountered: {results['errors']}", "[ERROR]")
        for file_path, error in results['error_files'][:5]:  # Show first 5 errors
            print_status(f"  {file_path}: {error}", "[ERROR]")
        if len(results['error_files']) > 5:
            print_status(f"  ... and {len(results['error_files']) - 5} more errors", "[ERROR]")
    
    return results
